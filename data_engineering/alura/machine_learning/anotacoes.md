O curso √© voltado para aprender sobre "√Årvores de Decis√£o"

Na imagem est√° sendo mostrado uma arvore de decis√£o.
![alt text](image.png)

ALGORITMO
CART (Classification and regression tree)
![alt text](image-1.png)

Um exemplo de dados
![alt text](image-2.png)

Dentro de uma tabela existe alguns "ATRIBUTOS" que podem ser bom ou ruim para divis√£o da arvore.
Porem como sabemos o qu√£o boa s√£o esses "ATRIBUTOS"?
Existe algumas m√©tricas para calcular o √≠ndice de impureza GINI.
![alt text](image-3.png)
Basicamente essa m√©trica calcula o qu√£o puro √© um n√≥, quanto mais puro for o n√≥, siginifica que o n√≥ possuiu mais transa√ß√µes de uma mesma classe. 
A classe usada como exemplo foi fraude ou n√£o fraude, se por exemplo o n√≥ voltar 100% fraude, quer dizer que ele √© totalmente puro, pois conseguimos classificar todos em uma mesma classe.

E como fazer os calculos?
Calculo do primeiro n√≥ da esquerda SIM.
Primeiro os Pk
![alt text](image-4.png)
formula = p1 = 1/3 (1 = Fraude e 3 = Transa√ß√µes)
p1 = 0.33 (Dizendo que 33% s√£o fraudes)

p2 = 2/3 (2 = N√£o fraudes e 3 = Transa√ß√µes)
p2 = 0.66 (Dizendo que 66% n√£o s√£o fraudes)

O indice GINI calculcar primeiro cada n√≥ separadamente e depois fazemos um somatorio do indice da arvore.

Na imagem como exemplo mostra como √© feito o calculo da formula que nos devolve o resultado de 44% impuro. 
0% √© um n√≥ puro, sendo uma divis√£o perfeita.
![alt text](image-5.png)
---
Calculo do segundo n√≥ da direita N√ÉO.
![alt text](image-6.png)
Agora calculcar o indice GINI
p1 = 1/2
p2 = 1/2

p1 = 0.50
p2 = 0.50

Forumula para p1 e p2
![alt text](image-7.png)
Resultado de 50% de impureza no n√≥ da direita.
---
Agora com os valores G de cada n√≥ (G=0.44 e G=0.50) para calculcar o GINI da arvore Gtree

Precisamos primeiro no peso de cada n√≥, que √© com base na QUANTIDADE de transa√ß√µes de cada n√≥.

![alt text](image-8.png)

fazemos a divis√£o 3/5 = 0.60 e 2/5 = 0.40

usamos a formula Gtree:
Gtree = (0.44 * 0.60) + (0.50 * 0.40)
Gtree = 0.26 + 0.20
Gtree = 0.46

Resultado que:
A arvore √© 46% impura.
----------------------------------------------------
![alt text](image-9.png)
Vamos escolher o GINI de menor valor, quanto menor o valor = melhor.

![alt text](image-10.png)
Podemos ver que do lado direito so tem 1 transa√ß√£o, ent√£o podemos colocar a classifica√ß√£o como FRAUDE.

Do lado esquerdo precisa fazer as contas do GINI.

![alt text](image-11.png)

Olhando para a arvore, foi decidido uma hierarquia de tamanho 2, quanto maior a arvoro, mais complexo.

![alt text](image-12.png)
No final analizando a arvore, faz sentido.
---------------------------------------------------
Alem de problemas de classifica√ß√£o, porem se tiver problema de REGRESS√ÉO?
Exemplo de dados:
![alt text](image-13.png)

Temos as propabilidades daquela transi√ß√£o ser uma fraude.

Primeiro passo √© dividir as trans√ß√µes no atributo pa√≠s, e agora vamos colocar as probabilidades de cada n√≥, depois √© calculcar a m√©dia de cada n√≥ (43% - 80%)
![alt text](image-14.png)

Dessa vez vamos utilizar a form√∫la de RSS (A soma dos quadrados dos res√≠diuos), quanto menor o valor/residuos √© mais proximo da m√©dia, esse √© o objetivo, significa que os valores represente aquele n√≥.
![alt text](image-15.png)

Depois precisamos somar todos os valores para chegar no valor de RSS da arvore.
![alt text](image-16.png)

Final dos resultados:
![alt text](image-17.png)
----------------------------------------------------------------------
1. temos que dividir os dados de treino (construir o modelo) e dados de teste (testar o modelo)
2. Como vimos que esta desbalanceados vamos utilizar um m√©todo do Scikit Learn, que √© chamado de Stratified Shuffle Split.

3. em vez de tirar um conjunto inteiro de dados, desse conjunto pegamos 10% pra teste e 90% pra treino.
4. E oque o metodo faz √© separar pela porcentagem 10% de teste e 90% de treino apenas do numero de fraudes, e faremos a mesma coisa para as transa√ß√µes normais 10% de teste e 90% de treino

5. Modelos preditivos servem para saber o que pode acontecer no futuro com base nos dados do passado, teste e treino servem para evitar overfitting, treino = O modelo aprende padr√µes e teste = modelo prova que aprendeu. Sem teste voc√™ n√£o sabe se o modelo generaliza.
6. Stratified mant√©m a propor√ß√£o de classes, pois cria aleatoriedade mantendo a propor√ß√£o. Porque se quase nenhuma √© fraude tanto com treino quanto com teste, o teste seria zero fraude.

√â descobrir padr√µes como:
Se V17 <= -2
E V14 <= -3
Ent√£o alta probabilidade de fraude

Isso √© regra de decis√£o.

Isso √© aprendizado supervisionado.

7. O modelo pode parecer perfeito, pode ter uma acurracy altissima, porem nao significa que aprendeu os padr√µes REAIS, significa que apenas memorizou os dados = OVERFITTING
Ele aprende o passado perfeitamente, porem quando aparece uma transa√ß√£o nova, ele falha.
√â como estudar exatamente as perguntas da prova
e depois pegar uma prova diferente.

8. Quando a √°rvore cresce sem limite em dados desbalanceados, ela tende a memorizar padr√µes raros da classe minorit√°ria, criando regras muito espec√≠ficas e levando ao overfitting, o que compromete a generaliza√ß√£o para novos dados.

√Årvore de decis√£o cresce at√©:
separar completamente os dados
ou at√© n√£o conseguir mais dividir

Em dados extremamente desbalanceados:
Ela pode criar regras muito espec√≠ficas como:

Se V17 <= -1.732
E V10 <= -5.123
E Time <= 123456.78
E Amount <= 345.12
E ...

Ela vai criando divis√µes cada vez mais espec√≠ficas.
Isso pode resultar em:

Gini = 0 em quase todos os n√≥s finais.

Treino ‚Üí perfeito
Teste ‚Üí pode cair bastante
-------------------------------------------------------------------------
üèó Arquitetura completa (do jeito certo)
1Ô∏è‚É£ Dados chegam

Transa√ß√µes chegam via:

API

Kafka

Banco relacional

Streaming


2Ô∏è‚É£ Data Lake (Bronze)

Dados crus, exatamente como chegaram.

Sem transforma√ß√£o.

S√≥ ingest√£o.

3Ô∏è‚É£ Silver (limpeza + padroniza√ß√£o)

Aqui acontece:

tratamento de nulos

normaliza√ß√£o

padroniza√ß√£o de tipos

remo√ß√£o de inconsist√™ncias

Agora os dados est√£o confi√°veis.

4Ô∏è‚É£ Feature Engineering (a parte que voc√™ perguntou)

Essa √© a parte MAIS importante para ML.

Feature engineering = criar novas vari√°veis √∫teis para o modelo.

Exemplos reais de fraude:

Quantidade de transa√ß√µes do cliente nas √∫ltimas 2 horas

Valor m√©dio das √∫ltimas 5 transa√ß√µes

Desvio padr√£o do valor

Dist√¢ncia geogr√°fica entre duas compras

Frequ√™ncia de compra noturna

Isso n√£o vem pronto no dataset.

Voc√™ cria.

E muitas vezes isso vale mais que o modelo.

üìå 80% do ML √© feature engineering.


5Ô∏è‚É£ Aplica√ß√£o do modelo

Voc√™ pega as features prontas e roda:

modelo.predict()

ou

modelo.predict_proba()

Agora voc√™ tem:

Probabilidade de fraude = 0.87

6Ô∏è‚É£ Tomada de decis√£o autom√°tica

Se probabilidade > 0.7:

bloqueia

pede valida√ß√£o

marca como suspeita

7Ô∏è‚É£ Log da decis√£o

Salva:

features usadas

probabilidade

decis√£o tomada

Isso √© importante para auditoria.

8Ô∏è‚É£ Power BI

Agora o BI mostra:

Fraudes detectadas hoje

Probabilidade m√©dia

Falsos positivos

Economia gerada

Agora BI vira monitoramento do modelo.
-------------------------------------------------------
Memorizar ‚Üí aprende regras extremamente espec√≠ficas dos dados vistos.

Aprender padr√£o ‚Üí aprende estrutura estat√≠stica que se mant√©m em novos dados.

Se n√£o houver fraude no teste, m√©tricas como Recall e Precision perdem significado.

10% de teste e 90% de treino - Teste mede generaliza√ß√£o
---------------------------------------------------------
1. treinou demais / armazenou padr√µes

Isso √© exatamente o ponto:

üëâ GINI baixo s√≥ prova que o modelo organizou bem o TREINO, n√£o que ele entende o mundo real.

Resumo mental r√°pido:

treino ‚Üí aprender regras

teste ‚Üí provar que n√£o decorou

Inst√°vel =
üëâ pequenas mudan√ßas nos dados causam grandes mudan√ßas nas m√©tricas.

E isso acontece quando:

‚úÖ existem pouqu√≠ssimos exemplos da classe importante.

üëâ Quanto MENOS exemplos da classe rara, MAIS inst√°veis s√£o as m√©tricas.

üëâ Quanto MAIS exemplos, MAIS confi√°vel √© a avalia√ß√£o.


Quando o teste possui apenas uma fraude, as m√©tricas ficam inst√°veis, pois um √∫nico erro ou acerto altera drasticamente o resultado, tornando a avalia√ß√£o do modelo pouco confi√°vel.
--------------------------------------------
Dataset desbalanceado N√ÉO √© o problema principal.
O verdadeiro problema √©:

üëâ avaliar o modelo com poucos exemplos da classe cr√≠tica.

Por isso existem t√©cnicas como:

Stratified Split ‚úÖ (voc√™ j√° viu)

Cross Validation

Oversampling (SMOTE)

Undersampling

Ajuste de threshold
-------------------------------------------
Accuracy alta n√£o quer dizer que o n√≥ foi puro

Cuidado aqui.

Accuracy n√£o mede pureza de n√≥.
Ela mede:

(acertos totais) / (total de previs√µes)
----------------------------------
Realidade	| Modelo disse	| Nome
Fraude	    | Fraude	    | ‚úÖ True Positive (TP)
N√£o fraude	| N√£o fraude	| ‚úÖ True Negative (TN)
N√£o fraude	| Fraude	    | ‚ö†Ô∏è False Positive (FP)
Fraude      | N√£o fraude	| üö® False Negative (FN)


o que REALMENTE era fraude ‚Üí y_test
o que o modelo ACHOU ‚Üí y_pred

Confusion Matrix (A BASE DE TUDO)
Quantas vezes o modelo acertou e errou?
[[28419   13]
 [   13   36]]

1. Primeira linha (compras normais)
28419 vezes: era normal - modelo disse normal - ‚úÖ ACERTO
13 vezes: era normal - modelo disse fraude - ‚ùå bloqueou cliente inocente

2. Segunda linha (fraudes)
13 vezes: era fraude - modelo disse normal - ‚ùå fraude passou
36 vezes: era fraude - modelo disse fraude - ‚úÖ salvou dinheiro

| Realidade | Modelo | Resultado           |
| --------- | ------ | ------------------- |
| Normal    | Normal | ‚úÖ                   |
| Normal    | Fraude | ‚ùå cliente bloqueado |
| Fraude    | Normal | ‚ùå preju√≠zo          |
| Fraude    | Fraude | ‚úÖ prote√ß√£o          |


2Ô∏è‚É£ Accuracy (Acur√°cia)
Conta TODOS os acertos:
28419 + 36
Divide pelo total.

Problema:
Como quase tudo √© normal,
acertar normal j√° d√° nota alta.
Por isso ela engana em fraude.

1) Tradu√ß√£o humana
Accuracy = nota geral da prova.
Mas a prova tem 99 perguntas f√°ceis e 1 dif√≠cil.

3Ô∏è‚É£ Precision (Precis√£o)
Pergunta:
Quando o modelo GRITOU ‚ÄúFRAUDE‚Äù, ele estava certo?

Olhamos s√≥ para os casos onde ele acusou fraude.
Entre esses:
quantos eram fraude de verdade?
Tradu√ß√£o humana
Precision mede:

üëâ quantos clientes inocentes voc√™ bloqueou sem querer.

Precision alta =
modelo s√≥ acusa quando tem certeza.

3. Quando a Precision fica baixa?

Quando o modelo vira paranoico:

üö® FRAUDE
üö® FRAUDE
üö® FRAUDE

Ele pega fraude‚Ä¶
mas tamb√©m bloqueia muita compra normal.

Resultado:

Clientes irritados.
----------------------------------
4Ô∏è‚É£ Recall (Sensibilidade)
Pergunta:
Das fraudes que EXISTIAM, quantas eu consegui pegar?

Aqui olhamos todas as fraudes reais.
E vemos quantas o modelo capturou.

Tradu√ß√£o humana
Recall mede:

üëâ quanto dinheiro voc√™ conseguiu salvar.

Recall alto =
poucas fraudes escapam.

4. Recall pergunta outra coisa:

Das fraudes que EXISTIAM, quantas eu consegui pegar?

Ent√£o:

Precision olha para os ALARMES

Recall olha para as FRAUDES REAIS
--------------------------------
üß© RESUMO DEFINITIVO
| M√©trica          | Pergunta humana             |
| ---------------- | --------------------------- |
| Confusion Matrix | O que exatamente aconteceu? |
| Accuracy         | Acertei no geral?           |
| Precision        | Bloqueei s√≥ quem devia?     |
| Recall           | Peguei as fraudes?          |
------------------------------------------------------------------

üéØ Visual definitivo (guarda isso)

Imagine:
Existiam 100 fraudes.

O modelo marcou 200 compras como fraude.

Entre essas 200:

80 eram fraude

120 eram normais

Ent√£o:

Precision:

80 / 200 = 40%


Recall:

80 / 100 = 80%


üëâ Pegou bastante fraude (recall bom)
üëâ Mas acusou muita gente inocente (precision ruim)
----------------------------------------------------------
üß† Consolida√ß√£o final (guarda isso)

Pensa sempre assim:

üîµ Recall

Olha para as fraudes reais.

Pergunta:

Eu consegui capturar elas?

Fraude escapando ‚Üí Recall baixo.

üü¢ Precision

Olha para os alarmes do modelo.

Pergunta:

Quando eu acusei fraude, eu estava certo?

Muitos clientes inocentes bloqueados ‚Üí Precision baixa.

üü° Accuracy

Olha tudo misturado.

Pergunta:

Acertei no geral?

Em fraude, quase sempre engana.

‚ö´ Confusion Matrix

√â o ‚Äúplacar completo‚Äù.
Todas as m√©tricas nascem dela.
----------------------------------------------------------

2. A grande ideia: Ensemble Learning

Aqui vem o pulo do gato.

Em vez de confiar em UMA √°rvore, fazemos:

v√°rias √°rvores pequenas trabalhando juntas.

Pensa assim:

üë§ 1 especialista ‚Üí pode errar
üë• 100 especialistas ‚Üí decis√£o mais confi√°vel

Isso √© Ensemble Learning.
---
3. Bagging (Bootstrapping + Aggregating)

Agora entra o conceito mais importante da aula.

‚úÖ Bootstrapping (amostragem)

O algoritmo pega o dataset e cria v√°rios subconjuntos aleat√≥rios:

Dataset original:
1 2 3 4 5 6 7 8 9 10 ...

Amostra A ‚Üí [1,12,17,9,14]
Amostra B ‚Üí [1,3,11,4,5]
Amostra C ‚Üí [3,7,16,17,20]

Cada amostra cria uma √°rvore diferente.

üëâ Cada √°rvore aprende uma vis√£o diferente do problema.
---
Aggregating (agrega√ß√£o)

Depois todas votam:

√Årvore 1 ‚Üí FRAUDE
√Årvore 2 ‚Üí N√ÉO FRAUDE
√Årvore 3 ‚Üí FRAUDE

Resultado final:

üëâ maioria vence = FRAUDE

Isso reduz muito erro individual.
---
4. Random Forest (a floresta)

Random Forest = Bagging + aleatoriedade extra.

Cada √°rvore:

‚úÖ usa transa√ß√µes aleat√≥rias
‚úÖ usa atributos aleat√≥rios (pa√≠s, valor, m√©dia‚Ä¶)

Ent√£o nenhuma √°rvore fica igual √† outra.

Visualmente:
        üå≥
     üå≥ üå≥ üå≥
   üå≥ üå≥ üå≥ üå≥

Cada √°rvore √© fraca sozinha.

Mas juntas ‚Üí modelo forte.
---
5. O que s√£o Out-of-Bag (OOB)?

Isso √© MUITO importante.

Quando uma √°rvore √© criada:

ela usa s√≥ parte dos dados

os dados n√£o usados viram teste autom√°tico

Exemplo:

√Årvore A1 treinou com:
[1,2]

Ent√£o valida com:
[3,4,5]

Isso chama:

üëâ Out of Bag samples

√â como ter um mini validation set gr√°tis.
---
6. O voto final (parte da tabela)

Na aula:

Transa√ß√£o	A1	A2	Resultado
5	SIM	SIM	‚úÖ SIM

As √°rvores votam.

Random Forest decide pela maioria.
---
7. Por que Random Forest melhora m√©tricas?

Porque ele:

‚úÖ reduz overfitting
‚úÖ reduz vari√¢ncia
‚úÖ evita decis√µes extremas de uma √∫nica √°rvore

Resultado da aula:

Modelo	Recall
√Årvore simples	0.73
Random Forest	0.75

Pequena melhora, mas muito mais robusto.
---
8. A √°rvore tenta achar:

"qual regra perfeita?"

Random Forest pensa:

"qual decis√£o √© consistente entre v√°rias vis√µes?"

Isso √© exatamente o que modelos reais fazem.
---
Decision Tree
‚Üí aprende regras

Overfit
‚Üí √°rvore profunda demais

Bagging
‚Üí v√°rias √°rvores com amostras diferentes

Random Forest
‚Üí v√°rias √°rvores + atributos aleat√≥rios

Resultado
‚Üí vota√ß√£o final mais est√°vel